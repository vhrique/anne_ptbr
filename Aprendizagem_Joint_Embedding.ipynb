{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdhDCuhShcmLz7I6rrd5XI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/Aprendizagem_Joint_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joint-embedding Learning\n",
        "\n",
        "O joint-embedding learning é uma abordagem que busca projetar diferentes modalidades de dados, como texto e imagens, em um espaço de representação compartilhado. O objetivo é permitir que dados de diferentes tipos sejam comparáveis diretamente, mesmo que venham de modalidades distintas. Esse processo de aprendizado é amplamente utilizado em modelos como o CLIP (Contrastive Language-Image Pre-training), que mapeia texto e imagens para o mesmo espaço vetorial, permitindo que descrições textuais e imagens correspondentes fiquem próximas entre si nesse espaço (Ramesh et al., 2022). Essa técnica possibilita tarefas como a busca cruzada entre modalidades, onde, por exemplo, pode-se encontrar uma imagem a partir de uma descrição textual ou vice-versa. Outra aplicação de joint-embedding é a criação de espaços semânticos que facilitam o zero-shot learning, permitindo que o modelo generalize para tarefas ou classes não vistas durante o treinamento. A principal vantagem do joint-embedding learning é a criação de uma representação comum que captura as relações semânticas entre diferentes formas de dados, tornando o aprendizado multimodal mais eficiente e flexível.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/clip.png?raw=true\" width=700></center>\n"
      ],
      "metadata": {
        "id": "AEzpWELOfqAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formas\n",
        "\n",
        "Existem diferentes tipos de joint-embedding que variam conforme a forma como as representações de diferentes modalidades são combinadas no espaço compartilhado. Um dos tipos mais comuns é o joint-embedding contrastivo, utilizado em modelos como o CLIP, onde as representações de modalidades (como imagens e textos) são aprendidas em paralelo e forçadas a se alinhar no mesmo espaço por meio de loss functions contrastivas. Esse método garante que as representações de itens correspondentes, como uma imagem e sua legenda, estejam próximas no espaço de embedding, enquanto itens não relacionados ficam distantes.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/joint_embedding.jpg?raw=true\" width=400></center>\n",
        "\n",
        "Outro tipo é o joint-embedding supervisionado, onde as modalidades são projetadas para um espaço compartilhado usando rótulos explícitos durante o treinamento, ajudando o modelo a aprender uma correspondência entre modalidades supervisionada por dados rotulados.\n",
        "\n",
        "Há também o joint-embedding não supervisionado, onde o modelo aprende a alinhar diferentes modalidades de forma autônoma, sem depender de rótulos, explorando padrões intrínsecos nos dados.\n",
        "\n",
        "Esses diferentes tipos de joint-embedding são amplamente aplicados em tarefas como busca multimodal, classificação cross-modal, e zero-shot learning, em que a flexibilidade do modelo para integrar informações de diferentes modalidades no mesmo espaço é crítica para seu sucesso."
      ],
      "metadata": {
        "id": "fpWfdaB5sra_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2), 3."
      ],
      "metadata": {
        "id": "bjYeSzfhkZQR"
      }
    }
  ]
}