# Fun√ß√µes de Perda

As fun√ß√µes de perda (ou fun√ß√µes de custo) s√£o componentes centrais no treinamento de redes neurais, pois medem o qu√£o bem o modelo est√° performando.
Elas comparam as predi√ß√µes feitas pela rede com os valores reais e retornam um valor num√©rico que indica o erro da previs√£o.
O objetivo do treinamento √© minimizar essa fun√ß√£o de perda ajustando os pesos da rede, com a ajuda de algoritmos de otimiza√ß√£o, para que o erro se torne o menor poss√≠vel.

Essencialmente, a fun√ß√£o de perda informa √† rede neural como melhorar suas predi√ß√µes ao longo do processo de aprendizado.
Dependendo do tipo de problema (classifica√ß√£o, regress√£o, multiclasse, etc.), diferentes fun√ß√µes de perda s√£o utilizadas.

## Fun√ß√µes de Perda para Classifica√ß√£o

### Binary Cross-Entropy

A Binary Cross-Entropy (BCE) √© comumente usada em problemas de classifica√ß√£o bin√°ria, onde o objetivo √© prever se uma amostra pertence a uma de duas classes.
Essa fun√ß√£o mede a diferen√ßa entre a probabilidade prevista e o valor real, penalizando fortemente predi√ß√µes erradas. A f√≥rmula √©:

$$
\text{BCE} = - \frac{1}{N}\sum_{i=1}^{N}\left[y_i.log(\hat{y}_i) + (1 - y_i).log(1-\hat{y}_i)\right]
$$

Aqui, $y_i$ √© o valor real (0 ou 1) e $\hat{y}_i$ √© a probabilidade prevista.
O objetivo √© minimizar essa diferen√ßa, fazendo com que a probabilidade prevista se aproxime do valor real.

- Vantagem: Funciona muito bem com problemas bin√°rios, lidando com probabilidades.
- Desvantagem: Pode ser mais sens√≠vel a problemas de balan√ßo de classes.

#### Multilabel Binary Cross-Entropy

Em problemas de classifica√ß√£o multilabel, onde uma entrada pode pertencer a mais de uma classe, a Multilabel Binary Cross-Entropy √© utilizada.
√â basicamente a BCE aplicada a cada r√≥tulo individualmente.
A f√≥rmula √© semelhante √† da BCE, mas ajustada para v√°rias sa√≠das simult√¢neas.

- Vantagem: Adequada para problemas onde uma entrada pertence a m√∫ltiplas classes.
- Desvantagem: Pode se tornar ineficiente com um grande n√∫mero de classes.

### Categorical Cross-Entropy

A Entropia Cruzada Categ√≥rica √© usada em problemas de classifica√ß√£o multiclasse, onde o objetivo √© prever uma entre v√°rias classes mutuamente exclusivas.
A f√≥rmula √©:

$$
\text{CCE} = - \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C}y_{ij}.log(\hat{y}_{ij})
$$

Aqui, $C$ √© o n√∫mero de classes, $y_{ij}$ √© o valor real (geralmente um vetor one-hot) e $\hat{y}_{ij}$ √© a probabilidade prevista para a classe $j$.
A _softmax_ √© usada como fun√ß√£o de ativa√ß√£o na sa√≠da, convertendo as predi√ß√µes em probabilidades.

- Vantagem: Adequada para problemas com v√°rias classes exclusivas.
- Desvantagem: N√£o lida bem com situa√ß√µes em que uma inst√¢ncia pode pertencer a m√∫ltiplas classes.

### Margin (Hinge) Loss

O Hinge Loss √© usado com m√°quinas de vetores de suporte (SVMs), mas tamb√©m pode ser aplicado em redes neurais para problemas de classifica√ß√£o bin√°ria.
Ele for√ßa o modelo a maximizar a margem entre as classes, penalizando erros de classifica√ß√£o de forma mais agressiva.

A f√≥rmula para um problema bin√°rio √©:

$$
L = \sum_{i=1}^{N}\text{max}(0,1-y_i.\hat{y}_i)
$$

Aqui, $y_i$ s√£o os r√≥tulos reais (+1 ou -1), $\hat{y}_{i}$ e s√£o as predi√ß√µes do modelo.

- Vantagem: Eficaz em maximizar a separa√ß√£o entre as classes.
- Desvantagem: Principalmente usado em SVMs e menos comum em redes neurais.

## Fun√ß√µes de Perda para Problemas de Regress√£o

### Mean Squared Error

O MSE √© amplamente usado em problemas de regress√£o, onde o objetivo √© prever valores cont√≠nuos.
Ele mede a diferen√ßa m√©dia entre as predi√ß√µes do modelo e os valores reais, elevando ao quadrado essas diferen√ßas para garantir que erros positivos e negativos n√£o se cancelem.
A f√≥rmula √©:

$$
\text{MSE} = \frac{1}{N}\sum_{i=1}^N(y_i - \hat{y}_i)^2
$$

onde $y_i$ √© o valor real e $\hat{y}_i$ √© a predi√ß√£o do modelo.
O quadrado das diferen√ßas garante que erros grandes tenham um impacto maior no valor final.

- Vantagem: Simples de calcular e amplifica grandes erros.
- Desvantagem: Sens√≠vel a outliers, j√° que erros grandes t√™m um impacto desproporcional.

### Mean Absolute Error

O MAE √© outra fun√ß√£o de perda usada para problemas de regress√£o, mas, ao contr√°rio do MSE, mede a diferen√ßa absoluta m√©dia entre os valores previstos e os valores reais.
Sua f√≥rmula √©:

$$
\text{MSE} = \frac{1}{N}\sum_{i=1}^N \left|y_i - \hat{y}_i \right|
$$

- Vantagem: Mais robusto a outliers do que o MSE.
- Desvantagem: N√£o diferencia grandes e pequenos erros da mesma forma que o MSE.

### Huber Loss

A Huber Loss √© uma fun√ß√£o de perda que combina as vantagens do Erro Quadr√°tico M√©dio (MSE) e do Erro Absoluto M√©dio (MAE), oferecendo uma abordagem robusta para problemas de regress√£o, especialmente na presen√ßa de outliers.
Ela foi projetada para tratar grandes erros de forma mais eficiente do que o MSE, que √© altamente sens√≠vel a outliers, enquanto mant√©m a simplicidade do MAE em regi√µes de pequenos erros.
A f√≥rmula da Huber Loss √© definida de forma diferente para erros pequenos e grandes:

$$
L_{\delta}(y_i, \hat{y}_i) =
\begin{cases}
\frac{1}{2}(y_i - \hat{y}_i)^2 & \text{se } |y_i - \hat{y}_i| \leq \delta \\
\delta \cdot (|y_i - \hat{y}_i| - \frac{1}{2} \delta) & \text{se } |y_i - \hat{y}_i| > \delta
\end{cases}
$$

onde $y_i$ √© o valor real, $\hat{y}_i$ √© a previs√£o do modelo e $\delta$ √© um par√¢metro que define o limite entre erros pequenos e grandes.

A Huber Loss funciona de forma suave em rela√ß√£o a pequenos erros, como o MSE, mas trata erros grandes de maneira mais robusta, como o MAE.
Isso faz com que seja uma fun√ß√£o intermedi√°ria que lida bem tanto com ru√≠dos pequenos quanto com outliers, sendo especialmente √∫til em problemas de regress√£o.

Vantagens:

- Robustez a outliers: Quando h√° grandes erros ou outliers, a Huber Loss n√£o amplifica esses erros tanto quanto o MSE, evitando que um pequeno n√∫mero de outliers distor√ßa significativamente o modelo.
- Suavidade em pequenos erros: Para erros pequenos, a Huber Loss se comporta como o MSE, permitindo que a fun√ß√£o de perda seja diferenci√°vel e suave, o que facilita a otimiza√ß√£o.

Desvantagens:

- Escolha de $\delta$: O valor de
$\delta$ deve ser escolhido cuidadosamente, pois um valor mal ajustado pode levar a um comportamento inadequado da fun√ß√£o de perda. Sefor muito pequeno, o modelo se comportar√° quase como o MAE, e se for muito grande, se aproximar√° do MSE, perdendo as vantagens da robustez

A Huber Loss √© amplamente utilizada em problemas de regress√£o onde h√° a presen√ßa de outliers nos dados, pois oferece um equil√≠brio entre ser suave para erros pequenos e robusta para grandes erros. Al√©m disso, √© preferida em aplica√ß√µes de machine learning onde o MSE tende a ser excessivamente influenciado por grandes outliers.

## Outras Fun√ß√µes de Perda

A KL Divergence √© utilizada para medir a diferen√ßa entre duas distribui√ß√µes de probabilidade, sendo particularmente √∫til em modelos probabil√≠sticos, como variational autoencoders (VAEs).
Ela n√£o √© propriamente uma fun√ß√£o de perda no sentido tradicional, mas √© amplamente usada para regularizar a distribui√ß√£o de sa√≠da de um modelo para que seja similar a uma distribui√ß√£o-alvo.

A Cosine Similarity Loss mede o √¢ngulo entre dois vetores, sendo usada para comparar a similaridade entre vetores em vez de medir a diferen√ßa absoluta.
Isso √© especialmente √∫til em problemas de aprendizado de representa√ß√µes, como em redes neurais siamesas e embedding learning.

A Focal Loss foi introduzida para tratar o problema de desbalanceamento de classes, em que algumas classes s√£o muito mais representadas que outras.
Ela √© uma modifica√ß√£o da entropia cruzada que coloca mais peso nas amostras dif√≠ceis (ou seja, aquelas que s√£o classificadas de forma incorreta).

Tamb√©m chamada de Huber Loss, mas com uma implementa√ß√£o ligeiramente diferente, a Smooth L1 Loss √© amplamente utilizada em redes neurais para detec√ß√£o de objetos. Ela √© mais robusta a outliers do que a MSE e mais est√°vel do que o MAE.

A Dice Loss √© uma fun√ß√£o de perda comumente usada em problemas de segmenta√ß√£o de imagens, especialmente em √°reas m√©dicas, onde √© necess√°rio calcular a sobreposi√ß√£o entre a √°rea predita e a √°rea verdadeira.
√â derivada do coeficiente de Dice, que mede a similaridade entre dois conjuntos.

A Triplet Loss √© usada para aprendizado de embeddings e aprendizado m√©trico, particularmente em redes siamesas.
A fun√ß√£o de perda incentiva a rede a reduzir a dist√¢ncia entre embeddings de amostras semelhantes (o ancor e o positivo) e aumentar a dist√¢ncia entre o ancor e amostras dissimilares (o negativo).

A Wing Loss √© uma fun√ß√£o de perda desenhada especificamente para problemas de detec√ß√£o de landmarks faciais, onde pequenos erros precisam ser suavemente penalizados, mas grandes erros devem ser penalizados de forma mais forte.

## Considera√ß√µes Finais: Cross-Entropy como Fun√ß√£o de Perda em Redes Neurais

A entropia cruzada (cross-entropy) tem suas origens na teoria da informa√ß√£o, desenvolvida por Claude Shannon na d√©cada de 1940.
A ideia central da teoria da informa√ß√£o √© quantificar a quantidade de informa√ß√£o ou incerteza presente em um conjunto de dados, e a entropia √© uma medida dessa incerteza.
A entropia de Shannon mede o grau de imprevisibilidade de um sistema de eventos, sendo usada para descrever a incerteza associada a uma distribui√ß√£o de probabilidades.

No contexto de redes neurais, a entropia cruzada √© uma medida da diverg√™ncia entre duas distribui√ß√µes de probabilidade: a distribui√ß√£o verdadeira dos r√≥tulos (a sa√≠da correta) e a distribui√ß√£o prevista pelo modelo.
A f√≥rmula original da entropia cruzada √© baseada na diverg√™ncia de Kullback-Leibler (KL Divergence), que mede a diferen√ßa entre duas distribui√ß√µes de probabilidade $P$ e $Q$.
No caso de uma rede neural, $P$ √© a distribui√ß√£o verdadeira dos r√≥tulos (normalmente representada por um vetor one-hot) e $Q$ √© a distribui√ß√£o de probabilidade prevista pelo modelo.

A f√≥rmula da entropia cruzada √© dada por:

$$
H(P,Q) = - \sum_{i=1}^{C}P(i)log(Q(i))
$$

Onde:

- $P(i)$ √© a probabilidade verdadeira para a classe $i$ (em problemas de classifica√ß√£o, geralmente 0 ou 1).
- $Q(i)$ √© a probabilidade prevista pelo modelo para a classe $i$.

A entropia cruzada mede o qu√£o bem o modelo est√° capturando a distribui√ß√£o verdadeira dos r√≥tulos.
Quando $P(i)$ √© 1 (ou seja, a classe $ùëñ$ √© a correta), a entropia cruzada penaliza o modelo se a probabilidade $ùëÑ(i)$ n√£o estiver pr√≥xima de 1.

