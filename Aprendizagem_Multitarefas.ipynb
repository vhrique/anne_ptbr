{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2aUj1CXJKQqqeup5tlJ/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/Aprendizagem_Multitarefas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-task Learning\n",
        "\n",
        "O multi-task learning (MTL) é um paradigma que visa resolver múltiplas tarefas simultaneamente, utilizando um único modelo. A principal motivação por trás do MTL é o compartilhamento de informação entre tarefas relacionadas, onde as representações aprendidas em uma tarefa podem beneficiar outras, levando a uma melhor generalização e redução do overfitting (Caruana, 1997). Ao compartilhar representações internas, o MTL permite que o modelo capte padrões comuns entre as tarefas, o que muitas vezes resulta em um aprendizado mais robusto e eficiente. Além disso, o MTL também proporciona eficiência computacional, pois evita a necessidade de treinar modelos separados para cada tarefa, o que pode reduzir significativamente o custo de treinamento e a complexidade do sistema. Este paradigma é amplamente utilizado em diversas áreas, como visão computacional e processamento de linguagem natural, onde tarefas inter-relacionadas podem compartilhar características comuns."
      ],
      "metadata": {
        "id": "zr6_LCzjfkXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formas\n",
        "\n",
        "O multi-task learning (MTL) pode ser realizado de diversas maneiras, dependendo de como as tarefas são integradas e de como as representações são compartilhadas. Uma das abordagens mais comuns é a de compartilhamento tardio de parâmetros, onde as tarefas compartilham as primeiras camadas do modelo para aprender representações gerais, e as camadas finais são específicas para cada tarefa. Outra abordagem é o compartilhamento antecipado, onde as tarefas têm redes praticamente separadas desde o início.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/multitask.jpg?raw=true\" width=400></center>\n",
        "\n",
        "\n",
        "Além disso, existem abordagens com redes completamente separadas que utilizam mecanismos para alinhar o aprendizado entre as tarefas, permitindo que troquem informações indiretamente, como as cross-stitch networks (Misra, 2016), ajustando as representações com base em seus respectivos contextos. Essas diferentes formas de realizar MTL são escolhidas dependendo da relação entre as tarefas e da necessidade de personalização das representações para cada uma.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/cross_stitch.jpg?raw=true\" width=400></center>"
      ],
      "metadata": {
        "id": "dtBEmT0sfvUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplos e Aplicações\n",
        "\n",
        "Multi-task Learning permite o desenvolvimento de diversas aplicações. De forma simplificada, um exemplo simples pode ser considerado o multi-label classification, já que a detecção de diferentes classes são realizadas simultaneamente. Porém, costumamos utilizar o termo de MTL para tarefas mais complexas.\n",
        "\n",
        "Um grande exemplo de MTL é a detecção de objetos, visto que temos saídas nas redes neurais para classificação e regressão (bounding boxes). Também temos modelos que realizam detecção e segmentação de objetos simultâneos, como Mask-RCNN, caracterizando aplicações de MTL.\n",
        "\n",
        "Além de visão computacional, também é possível realizar MTL ao treinar redes de processamento de linguagem natural para  analisar sentimentos e classificar tópicos simultaneamente. Em séries temporais, podemos realizar previsão de diversas saídas, como previsão de demanda e preço de produtos, simultaneemente.\n",
        "\n",
        "Aplicações mais recentes, como o desenvolvimento de veículos autônomos, envolvem não apenas realização de diversas tarefas, mas também uso de diversas modalidades de dados. Para isto, começamos também a trabalhar com Multi-modal Learning."
      ],
      "metadata": {
        "id": "dsT5WwO0f2od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercícios\n",
        "\n",
        "1. Volte ao exemplo de séries temporais da aula 4 (4.a) e explique como os modelos definidos trabalham com o conceito de multi-task learning\n",
        "2. Ao invés de um modelo multi-task, separe em dois modelos, um para GR e outro para CO2. Compare o resultado com a abordagem multi-task original.\n",
        "3. Explore modificações de arquitetura utilizando compartilhamento tardio (late sharing)."
      ],
      "metadata": {
        "id": "yIE6NhCbMd6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Caruana, R. (1997). Multitask learning. Machine learning, 28, 41-75.\n",
        "- Misra, I., Shrivastava, A., Gupta, A., & Hebert, M. (2016). Cross-stitch networks for multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3994-4003)."
      ],
      "metadata": {
        "id": "bjYeSzfhkZQR"
      }
    }
  ]
}